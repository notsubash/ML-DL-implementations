{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification with NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/subashpandey/Desktop/ML-DL-From-Scratch/data/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "mnist_train_dataset = torchvision.datasets.MNIST(root=image_path, train=True, transform=transform, download=False)\n",
    "mnist_test_dataset = torchvision.datasets.MNIST(root=image_path, train=False, transform=transform, download=False)\n",
    "batch_size = 64\n",
    "torch.manual_seed(1)\n",
    "train_dl = DataLoader(mnist_train_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=32, bias=True)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (4): ReLU()\n",
       "  (5): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_units = [32,16]\n",
    "image_size = mnist_train_dataset[0][0].shape\n",
    "input_size = image_size[0] * image_size[1] * image_size[2]\n",
    "all_layers = [nn.Flatten()]\n",
    "for hidden_unit in hidden_units:\n",
    "    layer = nn.Linear(input_size, hidden_unit)\n",
    "    all_layers.append(layer)\n",
    "    all_layers.append(nn.ReLU())\n",
    "    input_size = hidden_unit\n",
    "all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "model = nn.Sequential(*all_layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model starts with a flatten layer that flattens an input image into\n",
    "a one-dimensional tensor. This is because the input images are in the shape of\n",
    "[1, 28, 28]. The model has two hidden layers, with 32 and 16 units respectively.\n",
    "And it ends with an output layer of ten units representing ten classes, activated\n",
    "by a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Accuracy 0.8531\n",
      "Epoch 1  Accuracy 0.9287\n",
      "Epoch 2  Accuracy 0.9413\n",
      "Epoch 3  Accuracy 0.9506\n",
      "Epoch 4  Accuracy 0.9558\n",
      "Epoch 5  Accuracy 0.9592\n",
      "Epoch 6  Accuracy 0.9627\n",
      "Epoch 7  Accuracy 0.9649\n",
      "Epoch 8  Accuracy 0.9673\n",
      "Epoch 9  Accuracy 0.9690\n",
      "Epoch 10  Accuracy 0.9711\n",
      "Epoch 11  Accuracy 0.9729\n",
      "Epoch 12  Accuracy 0.9737\n",
      "Epoch 13  Accuracy 0.9747\n",
      "Epoch 14  Accuracy 0.9766\n",
      "Epoch 15  Accuracy 0.9778\n",
      "Epoch 16  Accuracy 0.9780\n",
      "Epoch 17  Accuracy 0.9798\n",
      "Epoch 18  Accuracy 0.9807\n",
      "Epoch 19  Accuracy 0.9815\n"
     ]
    }
   ],
   "source": [
    "# training, evaluation, and prediction\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "torch.manual_seed(1)\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    accuracy_hist_train = 0\n",
    "    for x_batch, y_batch in train_dl:\n",
    "        pred = model(x_batch)\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "        accuracy_hist_train += is_correct.sum()\n",
    "    accuracy_hist_train /= len(train_dl.dataset)\n",
    "    print(f'Epoch {epoch}  Accuracy '\n",
    "          f'{accuracy_hist_train:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9647\n"
     ]
    }
   ],
   "source": [
    "pred = model(mnist_test_dataset.data / 255.)\n",
    "is_correct = (torch.argmax(pred, dim=1) == mnist_test_dataset.targets).float()\n",
    "print(f\"Test accuracy: {is_correct.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification using PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subashpandey/Library/Caches/pypoetry/virtualenvs/subash-pandey-jvfq-machine-learning-new-vz-Rrc3MWra-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "        \n",
    "        # new PL attributes:\n",
    "\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.valid_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "        \n",
    "        # Model similar to previous section:\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2] \n",
    "        all_layers = [nn.Flatten()]\n",
    "        for hidden_unit in hidden_units: \n",
    "            layer = nn.Linear(input_size, hidden_unit) \n",
    "            all_layers.append(layer) \n",
    "            all_layers.append(nn.ReLU()) \n",
    "            input_size = hidden_unit \n",
    " \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10)) \n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Conditionally define epoch end methods based on PyTorch Lightning version\n",
    "    def on_training_epoch_end(self):\n",
    "        self.log(\"train_acc\", self.train_acc.compute())\n",
    "        self.train_acc.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute())\n",
    "        self.valid_acc.reset()\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.log(\"test_acc\", self.test_acc.compute())\n",
    "        self.test_acc.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = nn.functional.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightiningDataModule\n",
    "from torch.utils.data import DataLoader    \n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "class MnistDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='/Users/subashpandey/Desktop/ML-DL-From-Scratch/data/'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path, download=True)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # stage is either 'fit', 'validate', 'test', or 'predict'\n",
    "        mnist_all = MNIST(root=self.data_path, train=True, transform=self.transform, download=False)\n",
    "\n",
    "        self.train, self.val = random_split(\n",
    "            mnist_all, [55000,5000], generator=torch.Generator().manual_seed(1)\n",
    "        )\n",
    "        self.test = MNIST(root=self.data_path, train=False, transform=self.transform, download=False)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "     return DataLoader(self.test, batch_size=64, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "mnist_dm = MnistDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/subashpandey/Library/Caches/pypoetry/virtualenvs/subash-pandey-jvfq-machine-learning-new-vz-Rrc3MWra-py3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:07<00:00, 114.78it/s, v_num=2, train_loss=0.248] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:07<00:00, 114.68it/s, v_num=2, train_loss=0.248]\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(max_epochs=10, gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 86907), started 0:06:28 ago. (Use '!kill 86907' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f60730eb49cb48ed\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f60730eb49cb48ed\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /Users/subashpandey/Desktop/ML-DL-From-Scratch/miniprojects/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/subashpandey/Library/Caches/pypoetry/virtualenvs/subash-pandey-jvfq-machine-learning-new-vz-Rrc3MWra-py3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/subashpandey/Library/Caches/pypoetry/virtualenvs/subash-pandey-jvfq-machine-learning-new-vz-Rrc3MWra-py3.12/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:419: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/860 [00:00<?, ?it/s, v_num=3, train_loss=0.684]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subashpandey/Library/Caches/pypoetry/virtualenvs/subash-pandey-jvfq-machine-learning-new-vz-Rrc3MWra-py3.12/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='valid_acc')` could not find the monitored key in the returned metrics: ['train_loss', 'epoch', 'step']. HINT: Did you call `log('valid_acc', value)` in the `LightningModule`?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:07<00:00, 118.75it/s, v_num=3, train_loss=0.0412] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:07<00:00, 118.73it/s, v_num=3, train_loss=0.0412]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "mnistclassifier = MultiLayerPerceptron()\n",
    "\n",
    "callbacks = [ModelCheckpoint(save_top_k=1, mode='max', monitor=\"valid_acc\")] # save top 1 model\n",
    "\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks, gpus=1)\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10, callbacks=callbacks)\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating the model using tensorboard\n",
    "trainer.test(model=mnistclassifier, datamodule=mnist_dm, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reusing the model\n",
    "model = MultiLayerPerceptron.load_from_checkpoint('/Users/subashpandey/Desktop/ML-DL-From-Scratch/miniprojects/lightning_logs/version_2/checkpoints/epoch=9-step=8600.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subash-pandey-jvfq-machine-learning-new-vz-Rrc3MWra-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
